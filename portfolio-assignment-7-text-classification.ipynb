{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Wyatt Kirby\n#IRK180000\n#CS 4395.001\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#importing what is needed for Naive Bayes\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/mental-health-corpus/mental_health.csv', encoding='utf-8')\nprint(df.head())\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-30T20:13:41.303891Z","iopub.execute_input":"2023-03-30T20:13:41.305023Z","iopub.status.idle":"2023-03-30T20:13:41.476765Z","shell.execute_reply.started":"2023-03-30T20:13:41.304958Z","shell.execute_reply":"2023-03-30T20:13:41.475486Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"                                                text  label\n0  dear american teens question dutch person hear...      0\n1  nothing look forward lifei dont many reasons k...      1\n2  music recommendations im looking expand playli...      0\n3  im done trying feel betterthe reason im still ...      1\n4  worried  year old girl subject domestic physic...      1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"With this dataset I should be able to classify some text as possibly being indicative of mental health issues or not. In the 'label' column, a 0 indicates that there no need to worry.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Removing the stop words from the data\nstopwords = set(stopwords.words('english'))\nvectorizer = TfidfVectorizer(stop_words=stopwords)\n\n#setting up X and Y\nx = df.text\ny = df.label","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:41.479584Z","iopub.execute_input":"2023-03-30T20:13:41.480027Z","iopub.status.idle":"2023-03-30T20:13:41.492946Z","shell.execute_reply.started":"2023-03-30T20:13:41.479991Z","shell.execute_reply":"2023-03-30T20:13:41.491236Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:41.494574Z","iopub.execute_input":"2023-03-30T20:13:41.495031Z","iopub.status.idle":"2023-03-30T20:13:41.504511Z","shell.execute_reply.started":"2023-03-30T20:13:41.494988Z","shell.execute_reply":"2023-03-30T20:13:41.503219Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0    dear american teens question dutch person hear...\n1    nothing look forward lifei dont many reasons k...\n2    music recommendations im looking expand playli...\n3    im done trying feel betterthe reason im still ...\n4    worried  year old girl subject domestic physic...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"y[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:41.506727Z","iopub.execute_input":"2023-03-30T20:13:41.507192Z","iopub.status.idle":"2023-03-30T20:13:41.517833Z","shell.execute_reply.started":"2023-03-30T20:13:41.507147Z","shell.execute_reply":"2023-03-30T20:13:41.516638Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0    0\n1    1\n2    0\n3    1\n4    1\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=.2, train_size=.8, random_state=1234)\n\nprint(x.shape)\nprint(xtrain.shape)\nprint(xtest.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:41.522858Z","iopub.execute_input":"2023-03-30T20:13:41.523188Z","iopub.status.idle":"2023-03-30T20:13:41.534994Z","shell.execute_reply.started":"2023-03-30T20:13:41.523156Z","shell.execute_reply":"2023-03-30T20:13:41.533615Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"(27977,)\n(22381,)\n(5596,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see the training set has 80% of the initial dataset and the testing sets contain 20%","metadata":{}},{"cell_type":"code","source":"xtrain = vectorizer.fit_transform(xtrain)\nxtest = vectorizer.transform(xtest)   ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:41.536508Z","iopub.execute_input":"2023-03-30T20:13:41.536904Z","iopub.status.idle":"2023-03-30T20:13:43.665336Z","shell.execute_reply.started":"2023-03-30T20:13:41.536841Z","shell.execute_reply":"2023-03-30T20:13:43.663940Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print('train size:', xtrain.shape)\nprint(xtrain.toarray()[:5])\n\nprint('\\ntest size:', xtest.shape)\nprint(xtest.toarray()[:5])","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:43.666863Z","iopub.execute_input":"2023-03-30T20:13:43.667751Z","iopub.status.idle":"2023-03-30T20:13:50.442338Z","shell.execute_reply.started":"2023-03-30T20:13:43.667715Z","shell.execute_reply":"2023-03-30T20:13:50.441097Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"train size: (22381, 63944)\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\ntest size: (5596, 63944)\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Naive Bayes:**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nbayes = MultinomialNB()\nbayes.fit(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.444027Z","iopub.execute_input":"2023-03-30T20:13:50.444366Z","iopub.status.idle":"2023-03-30T20:13:50.471812Z","shell.execute_reply.started":"2023-03-30T20:13:50.444332Z","shell.execute_reply":"2023-03-30T20:13:50.470505Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"MultinomialNB()"},"metadata":{}}]},{"cell_type":"code","source":"import math\nprior = sum(ytrain==1)/len(ytrain)\nprint('prior label:', prior, 'log of prior:', math.log(prior))\nbayes.class_log_prior_[1]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.473395Z","iopub.execute_input":"2023-03-30T20:13:50.473779Z","iopub.status.idle":"2023-03-30T20:13:50.486839Z","shell.execute_reply.started":"2023-03-30T20:13:50.473745Z","shell.execute_reply":"2023-03-30T20:13:50.485234Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"prior label: 0.4930521424422501 log of prior: -0.7071403449292926\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"-0.707140344929293"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\npred = bayes.predict(xtest)\n\nprint(confusion_matrix(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.488635Z","iopub.execute_input":"2023-03-30T20:13:50.489213Z","iopub.status.idle":"2023-03-30T20:13:50.502228Z","shell.execute_reply.started":"2023-03-30T20:13:50.489130Z","shell.execute_reply":"2023-03-30T20:13:50.501364Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[[2004  789]\n [  31 2772]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see, there were far more true negative and true positives than there were msitakes, meaning that Naive Bayes was farily accurate without needing more fine tuning.","metadata":{}},{"cell_type":"code","source":"print('accuracy score: ', accuracy_score(ytest, pred))\n      \nprint('\\nprecision score (no sign): ', precision_score(ytest, pred, pos_label=0))\nprint('precision score (sign): ', precision_score(ytest, pred))\n\nprint('\\nrecall score: (not sign)', recall_score(ytest, pred, pos_label=0))\nprint('recall score: (sign)', recall_score(ytest, pred))\n      \nprint('\\nf1 score: ', f1_score(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.503774Z","iopub.execute_input":"2023-03-30T20:13:50.504095Z","iopub.status.idle":"2023-03-30T20:13:50.528085Z","shell.execute_reply.started":"2023-03-30T20:13:50.504064Z","shell.execute_reply":"2023-03-30T20:13:50.526784Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"accuracy score:  0.8534667619728378\n\nprecision score (no sign):  0.9847665847665847\nprecision score (sign):  0.7784330244313395\n\nrecall score: (not sign) 0.7175080558539205\nrecall score: (sign) 0.9889404209775241\n\nf1 score:  0.8711502199874293\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Naive Bayes had a accuracy score of 85%, is more precise at identifying text that does not show signs of mental illness, and has a better recall for text that does show sign of mental illness.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.529992Z","iopub.execute_input":"2023-03-30T20:13:50.531100Z","iopub.status.idle":"2023-03-30T20:13:50.551348Z","shell.execute_reply.started":"2023-03-30T20:13:50.531061Z","shell.execute_reply":"2023-03-30T20:13:50.550114Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.98      0.72      0.83      2793\n           1       0.78      0.99      0.87      2803\n\n    accuracy                           0.85      5596\n   macro avg       0.88      0.85      0.85      5596\nweighted avg       0.88      0.85      0.85      5596\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Logistic Regression:**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(solver='lbfgs', class_weight='balanced')\nclassifier.fit(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:50.552592Z","iopub.execute_input":"2023-03-30T20:13:50.552895Z","iopub.status.idle":"2023-03-30T20:13:51.810218Z","shell.execute_reply.started":"2023-03-30T20:13:50.552866Z","shell.execute_reply":"2023-03-30T20:13:51.808614Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(class_weight='balanced')"},"metadata":{}}]},{"cell_type":"code","source":"pred = classifier.predict(xtest)\nprint(confusion_matrix(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:51.817584Z","iopub.execute_input":"2023-03-30T20:13:51.818624Z","iopub.status.idle":"2023-03-30T20:13:51.832346Z","shell.execute_reply.started":"2023-03-30T20:13:51.818558Z","shell.execute_reply":"2023-03-30T20:13:51.830835Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"[[2604  189]\n [ 281 2522]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This was was more accurate than the Naive Bayes model, as it identified more true positive and true negative. Had I tuned Bayes adn tried again it might have peformed similarly, but I think it's better to just run all of them once and compare how they all did initially.","metadata":{}},{"cell_type":"code","source":"print('accuracy score: ', accuracy_score(ytest, pred))\n      \nprint('\\nprecision score (no sign): ', precision_score(ytest, pred, pos_label=0))\nprint('precision score (sign): ', precision_score(ytest, pred))\n\nprint('\\nrecall score: (not sign)', recall_score(ytest, pred, pos_label=0))\nprint('recall score: (sign)', recall_score(ytest, pred))\n      \nprint('\\nf1 score: ', f1_score(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:51.834766Z","iopub.execute_input":"2023-03-30T20:13:51.835829Z","iopub.status.idle":"2023-03-30T20:13:51.877164Z","shell.execute_reply.started":"2023-03-30T20:13:51.835766Z","shell.execute_reply":"2023-03-30T20:13:51.875517Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"accuracy score:  0.916011436740529\n\nprecision score (no sign):  0.9025996533795494\nprecision score (sign):  0.9302840280339358\n\nrecall score: (not sign) 0.9323308270676691\nrecall score: (sign) 0.8997502675704602\n\nf1 score:  0.9147624229234675\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see here, the accuracy of Logistic Regression is 91%. All scored for logistic regression surpass those from the Bayes model.","metadata":{}},{"cell_type":"code","source":"print(classification_report(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:51.879756Z","iopub.execute_input":"2023-03-30T20:13:51.880852Z","iopub.status.idle":"2023-03-30T20:13:51.913052Z","shell.execute_reply.started":"2023-03-30T20:13:51.880788Z","shell.execute_reply":"2023-03-30T20:13:51.911517Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.93      0.92      2793\n           1       0.93      0.90      0.91      2803\n\n    accuracy                           0.92      5596\n   macro avg       0.92      0.92      0.92      5596\nweighted avg       0.92      0.92      0.92      5596\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Neural Network:**","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclassifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15, 2), random_state=1)\nclassifier.fit(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:51.915275Z","iopub.execute_input":"2023-03-30T20:13:51.915936Z","iopub.status.idle":"2023-03-30T20:13:55.602064Z","shell.execute_reply.started":"2023-03-30T20:13:51.915904Z","shell.execute_reply":"2023-03-30T20:13:55.600436Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15, 2), random_state=1,\n              solver='lbfgs')"},"metadata":{}}]},{"cell_type":"code","source":"pred = classifier.predict(xtest)\nprint(confusion_matrix(ytest, pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:55.604983Z","iopub.execute_input":"2023-03-30T20:13:55.606200Z","iopub.status.idle":"2023-03-30T20:13:55.627014Z","shell.execute_reply.started":"2023-03-30T20:13:55.606133Z","shell.execute_reply":"2023-03-30T20:13:55.625510Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[[2793    0]\n [2803    0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It seems as though the neural network only guessed that each text has no signs of mental illness. Lets check.","metadata":{}},{"cell_type":"code","source":"pred[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:55.629573Z","iopub.execute_input":"2023-03-30T20:13:55.630658Z","iopub.status.idle":"2023-03-30T20:13:55.641469Z","shell.execute_reply.started":"2023-03-30T20:13:55.630596Z","shell.execute_reply":"2023-03-30T20:13:55.639858Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see, this is correct for at least the first five. I mostly did this just to check which one the Neural Network was guessing.","metadata":{}},{"cell_type":"code","source":"print('accuracy score: ', accuracy_score(ytest, pred))\nprint('precision score: ', precision_score(ytest, pred, pos_label=0))\nprint('recall score: ', recall_score(ytest, pred, pos_label=0))\nprint('f1 score: ', f1_score(ytest, pred, pos_label=0))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:13:55.644170Z","iopub.execute_input":"2023-03-30T20:13:55.645573Z","iopub.status.idle":"2023-03-30T20:13:55.675125Z","shell.execute_reply.started":"2023-03-30T20:13:55.645507Z","shell.execute_reply":"2023-03-30T20:13:55.673534Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"accuracy score:  0.49910650464617584\nprecision score:  0.49910650464617584\nrecall score:  1.0\nf1 score:  0.6658719752056265\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This one had such low scores for accuracy because it only chose that the text showed no sign of mental illness. It really had 50 / 50 odds as there were only two options and the dataset seems to have been evenly split between the two. This means that the precision, recall, and f1 scores mean basically next to nothing.","metadata":{}},{"cell_type":"markdown","source":"# Analysis\n**Naive Bayes**\n\nThe naive bayes model performed the second best between all three. This model used a fairly small dataset and only had two options, making it a slightly easier model for me to train on. This being said, this model had a fairly high accuracy score of 85%. It also has a incredibly high precision score (91%) for predicting the text without worrying language, meaning that it made very few errors and was better at predicting those. I am unsure why there was such a drop with the other category (roughly 78%). This could be due to some grammatical errors, punctuation, or spelling errors that those in an unstable frame of mind might be more likely to make than someone who is feeling fine in that moment. These errors could throw off the model and make it harder to predict for those catagories. The recall was the opposite of the precision, having a higher percentage with the worrying text (99%) than the healthy text samples (78%). This may be due to the issues that caused a lower precision, as a person in crisis may have a more distincive writing style than that of someone who is fine. This would mean that out of all the text that is worrying, it would be easier to identify those, but might also include text that isn't worrying from someone who may not type well or have developed their own writing style when texting friends.\n\n**Logistic Regression**\n\nThis model performed the best out of all of the models in accuracy and had a more even distribution with in precision and recall. The accuracy was a whopping 92%, which is incredibly impressive considering that this is the initial run with no fine tuning. All of the scores for recall and precision were in the 90th percentile, so that means that not only was it likely that if it made a guess, that that guess was correct and that overall all the majority of the text was categorized correctly. I think that the distinctive writing styles that may have confused the Bayes model helped Logistic Regression make a more informed decision when catagorizing everything.\n\n**Neural Networks**\n\nThis one performed so poorly. Everything was very low. I tried tinkering with various input values, but it didn't make much of an impact at all (usually causing the performance overall to deteriorate further). The overall accuracy was at 50%, and looking at the confusion matrix its clear to see that the model only guessed that the text was normal and healthy. This means that everything was split at the middle, as we see with the accuracy and precision. Recall was high only because it guessed that all of the healthy text was healthy by default. The recall for the worrying messages was at zero. Overall dissapointing. In the machine learning class I took last year the Neural Networks seemed to perform better than the other algorithms, but maybe it struggles when spelling errors and grammatical errors are present.","metadata":{}}]}